{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T07:11:28.267077Z","iopub.status.busy":"2024-10-03T07:11:28.266607Z","iopub.status.idle":"2024-10-03T07:11:28.493051Z","shell.execute_reply":"2024-10-03T07:11:28.491885Z","shell.execute_reply.started":"2024-10-03T07:11:28.267029Z"},"trusted":true},"outputs":[],"source":["from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","token = user_secrets.get_secret(\"github_token\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T07:11:30.735656Z","iopub.status.busy":"2024-10-03T07:11:30.735190Z","iopub.status.idle":"2024-10-03T07:11:31.923453Z","shell.execute_reply":"2024-10-03T07:11:31.921658Z","shell.execute_reply.started":"2024-10-03T07:11:30.735610Z"},"trusted":true},"outputs":[],"source":["!git clone https://{token}@github.com/seprow/BrainMriClassification-IAAA-.git"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-10-03T07:15:34.485773Z","iopub.status.busy":"2024-10-03T07:15:34.485305Z","iopub.status.idle":"2024-10-03T07:19:48.498957Z","shell.execute_reply":"2024-10-03T07:19:48.497305Z","shell.execute_reply.started":"2024-10-03T07:15:34.485731Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["#!pip install -r '/kaggle/working/BrainMriClassification-IAAA-/requirements.txt'\n","!pip install SimpleITK\n","!pip install pydicom\n","!pip install albumentations\n","!pip install antspyx\n","!pip install plotly ipywidgets\n","!pip install monai"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T07:21:07.624399Z","iopub.status.busy":"2024-10-03T07:21:07.623801Z","iopub.status.idle":"2024-10-03T07:21:14.167613Z","shell.execute_reply":"2024-10-03T07:21:14.165910Z","shell.execute_reply.started":"2024-10-03T07:21:07.624348Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: nvidia-smi: command not found\n"]}],"source":["import pandas as pd\n","import os\n","import sys\n","from pathlib import Path\n","import SimpleITK as sitk\n","\n","sys.path.append('/kaggle/working/BrainMriClassification-IAAA-')\n","from Preprocess import *\n","from DataGenerator import *\n","\n","import torch\n","from torch.utils.data import DataLoader\n","torch.cuda.empty_cache() # Clears all the cache allocated by PyTorch on the GPU\n","\n","!nvidia-smi"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T07:22:26.582632Z","iopub.status.busy":"2024-10-03T07:22:26.582108Z","iopub.status.idle":"2024-10-03T07:22:26.589371Z","shell.execute_reply":"2024-10-03T07:22:26.588222Z","shell.execute_reply.started":"2024-10-03T07:22:26.582587Z"},"trusted":true},"outputs":[],"source":["ROOT_DATA_DIR = Path(r'/kaggle/input/iaaa-mri-challenge').expanduser().absolute()\n","DATA_DIR = os.path.join(ROOT_DATA_DIR,'data')\n","LABELS_PATH = os.path.join(ROOT_DATA_DIR,'train.csv')"]},{"cell_type":"markdown","metadata":{},"source":["# Data Handling"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T07:22:35.222138Z","iopub.status.busy":"2024-10-03T07:22:35.220937Z","iopub.status.idle":"2024-10-03T07:22:35.291483Z","shell.execute_reply":"2024-10-03T07:22:35.290109Z","shell.execute_reply.started":"2024-10-03T07:22:35.222084Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["prediction\n","0    2731\n","1     386\n","Name: count, dtype: int64\n"]}],"source":["annotations = pd.read_csv(LABELS_PATH)\n","list_id = [\n","    '1.3.46.670589.11.10042.5.0.7984.2024022316295067732',\n","    '1.3.46.670589.11.10042.5.0.8184.2024011321595084988',\n","    '1.3.46.670589.11.10042.5.0.5548.2024010521045198196',\n","    '1.3.46.670589.11.10042.5.0.6048.2024030612191717163',\n","    '1.3.46.670589.11.10042.5.0.3364.2024011206110656762',\n","    '1.3.46.670589.11.10042.5.0.5244.2024011800080517866',\n","    '1.3.46.670589.11.10042.5.0.5596.2024031317194490629',\n","    '1.3.46.670589.11.10042.5.0.5864.2024030418020328916',\n","    '1.3.46.670589.11.10042.5.0.6596.2024021917471807658',\n","    '1.3.46.670589.11.10042.5.0.5244.2024011514063439527',\n","    '1.3.46.670589.11.10042.5.0.7620.2023122711014093653',\n","    '1.3.46.670589.11.10042.5.0.6596.2024012110370925812',\n","    '1.3.46.670589.11.10042.5.0.5484.2024030209230264275',\n","    '1.3.46.670589.11.10042.5.0.1412.2024020409533565254',\n","    '1.3.46.670589.11.10042.5.0.6596.2024022109510462198'\n","]\n","\n","removable_id = annotations[annotations['SeriesInstanceUID'].isin(list_id)].index\n","annotations.drop(removable_id, axis=0, inplace=True)\n","\n","print(annotations['prediction'].value_counts())"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T07:22:36.454989Z","iopub.status.busy":"2024-10-03T07:22:36.454514Z","iopub.status.idle":"2024-10-03T07:22:36.793787Z","shell.execute_reply":"2024-10-03T07:22:36.792569Z","shell.execute_reply.started":"2024-10-03T07:22:36.454942Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[0.57067007, 4.0375648]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["from helper import estimate_class_weights \n","estimate_class_weights(annotations, method='mfb')"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T07:22:39.816157Z","iopub.status.busy":"2024-10-03T07:22:39.815670Z","iopub.status.idle":"2024-10-03T07:22:39.920870Z","shell.execute_reply":"2024-10-03T07:22:39.919165Z","shell.execute_reply.started":"2024-10-03T07:22:39.816110Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","validation_size = 0.3\n","\n","stratify_df = annotations['prediction']\n","training_df, validation_df = train_test_split(\n","    annotations,\n","    test_size=validation_size,\n","    stratify=annotations['prediction'],\n","    random_state=7\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#hyperparameters\n","target_size = [288,288,16]\n","batch_size = 8\n","\n","reference_image = {\n","    'T1W_SE': '/kaggle/input/iaaa-mri-challenge/data/1.3.46.670589.11.10042.5.0.1412.2024020321545257411',\n","    'T2W_TSE' : '/kaggle/input/iaaa-mri-challenge/data/1.3.46.670589.11.10042.5.0.1412.2024020313391873234',\n","    'T2W_FLAIR' : '/kaggle/input/iaaa-mri-challenge/data/1.3.46.670589.11.10042.5.0.1412.2024020410193570629',\n","}\n","\n","\n","# Dataset & Dataloder\n","train_dataset = DICOMDataGenerator(df=training_df, target_size=target_size, data_dir=DATA_DIR, reference_image_path=reference_image, shuffle=True)\n","val_dataset = DICOMDataGenerator(df=validation_df, target_size=target_size, data_dir=DATA_DIR, reference_image_path=reference_image,shuffle=False)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","# Create a sample batch\n","sample_batch = next(iter(train_loader))\n","images, labels = sample_batch\n","\n","print(f'Image batch shape: {images.shape}')\n","print(f'Label batch shape: {labels.shape}')"]},{"cell_type":"markdown","metadata":{},"source":["# Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from models import *\n","from helper import print_trainable_parameters\n","\n","model = SimpleVASNet()\n","print_trainable_parameters(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models\n","import numpy as np\n","from sklearn.metrics import roc_auc_score, recall_score, precision_score, accuracy_score, precision_recall_curve, auc, roc_curve\n","from torch.utils.tensorboard import SummaryWriter  \n","import logging  \n","from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Set PYTORCH_CUDA_ALLOC_CONF to reduce memory fragmentation\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n","\n","class Trainer:\n","    def __init__(self, train_data, val_data, model, num_epochs, lr, lr_decay_epoch, threshold=0.5, device=None, log_dir='logs'):\n","        self.train_data = train_data\n","        self.val_data = val_data\n","        self.model = model\n","        self.num_epochs = num_epochs\n","        self.lr = lr\n","        self.lr_decay_epoch = lr_decay_epoch\n","        self.best_val_loss = float('inf')\n","        self.threshold = threshold\n","\n","        # Device configuration\n","        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.model = self.model.to(self.device)\n","\n","        self.criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([6.3], dtype=torch.float32).to(self.device))\n","        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n","\n","        if os.path.exists('best_model.pth'):\n","            self.model.load_state_dict(torch.load('best_model.pth', map_location=self.device))\n","            print(\"Loaded best model from 'best_model.pth'\")\n","\n","\n","        # Initialize TensorBoard\n","        self.writer = SummaryWriter(log_dir=log_dir)\n","\n","        # Set up logging\n","        logging.basicConfig(filename=os.path.join(log_dir, 'training.log'), level=logging.INFO)\n","        self.logger = logging.getLogger()\n","\n","    def adjust_learning_rate(self, epoch):\n","        \"\"\"Decay the learning rate after a specific number of epochs.\"\"\"\n","        if epoch > self.lr_decay_epoch:\n","            for param_group in self.optimizer.param_groups:\n","                param_group['lr'] = self.lr * 0.5\n","\n","    def train_one_epoch(self):\n","        \"\"\"Train the model for one epoch.\"\"\"\n","        self.model.train()\n","        train_loss = 0.0\n","        all_train_targets = []\n","        all_train_outputs = []\n","\n","        with tqdm(self.train_data, desc=\"Training\") as pbar:  # Correct tqdm usage\n","            for inputs, targets in pbar:  # Indentation corrected\n","                inputs, targets = inputs.to(self.device), targets.to(self.device)\n","\n","                self.optimizer.zero_grad()\n","                outputs = self.model(inputs)\n","                loss = self.criterion(outputs, targets)\n","                loss.backward()\n","                self.optimizer.step()\n","\n","                train_loss += loss.item()\n","                all_train_targets.extend(targets.cpu().numpy())\n","                all_train_outputs.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n","                \n","                pbar.set_description(f\"Training (Loss: {train_loss / len(self.train_data):.4f})\")  # Fixed variable name\n","\n","        train_loss /= len(self.train_data)\n","        train_accuracy = accuracy_score(np.array(all_train_targets), np.array(all_train_outputs).round())\n","\n","        return train_loss, train_accuracy\n","\n","    def validate(self, threshold=0.4):\n","        \"\"\"Validate the model on the validation dataset\"\"\"\n","        self.model.eval()\n","        val_loss = 0.0\n","        all_val_targets = []\n","        all_val_outputs = []\n","\n","        with torch.no_grad():\n","            for inputs, targets in self.val_data:\n","                inputs, targets = inputs.to(self.device), targets.to(self.device)\n","\n","                outputs = self.model(inputs)\n","                loss = self.criterion(outputs, targets)\n","                val_loss += loss.item()\n","\n","                all_val_targets.extend(targets.cpu().numpy())\n","                all_val_outputs.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n","                \n","        # Apply the custom threshold to determine positive class predictions\n","        binary_predictions = (np.array(all_val_outputs) >= threshold).astype(int)\n","\n","        val_loss /= len(self.val_data)\n","        val_accuracy = accuracy_score(np.array(all_val_targets), binary_predictions)\n","        fpr, tpr, thresholds_roc = roc_curve(np.array(all_val_targets), np.array(all_val_outputs))\n","        roc_auc = auc(fpr, tpr)  # Fixed AUC naming conflict\n","        precision, recall, _ = precision_recall_curve(np.array(all_val_targets), np.array(all_val_outputs))\n","        pr_auc = auc(recall, precision)  # Fixed precision-recall AUC calculation\n","\n","        return val_loss, val_accuracy, roc_auc, pr_auc\n","\n","    def train_and_evaluate(self, threshold=0.4):\n","        \"\"\"Train and evaluate the model over multiple epochs.\"\"\"\n","        with tqdm(range(self.num_epochs), desc=\"Epoch\") as pbar:\n","            for epoch in pbar:\n","                self.adjust_learning_rate(epoch)\n","\n","                train_loss, train_accuracy = self.train_one_epoch()\n","                val_loss, val_accuracy, roc_auc, pr_auc = self.validate(threshold=threshold)\n","\n","                # Log metrics to TensorBoard\n","                self.writer.add_scalar('Loss/Train', train_loss, epoch)\n","                self.writer.add_scalar('Loss/Validation', val_loss, epoch)\n","                self.writer.add_scalar('Accuracy/Train', train_accuracy, epoch)\n","                self.writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)\n","                self.writer.add_scalar('AUC/Validation', roc_auc, epoch)\n","                self.writer.add_scalar('pr_auc/Validation', pr_auc, epoch)\n","\n","                # Log to the console and file\n","                log_message = (\n","                    f'Epoch {epoch+1}/{self.num_epochs}, '\n","                    f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, '\n","                    f'Validation Loss: {val_loss:.4f}, AUC: {roc_auc:.4f}, '\n","                    f'pr_auc: {pr_auc:.4f}, '\n","                    f'Validation Accuracy: {val_accuracy:.4f}'\n","                )\n","                print(log_message)\n","                self.logger.info(log_message)\n","\n","                # Save the best model\n","                if val_loss < self.best_val_loss:\n","                    self.best_val_loss = val_loss\n","                    torch.save(self.model.state_dict(), 'best_model.pth')\n","                    print(f'Best model saved with validation loss: {val_loss:.4f}')\n","                    self.logger.info(f'Best model saved with validation loss: {val_loss:.4f}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_epochs = 10\n","learning_rate = 0.0001\n","lr_decay_epoch = 10\n","threshold = 0.4\n","\n","trainer = Trainer(train_data=train_loader, val_data=val_loader, model=model, num_epochs=num_epochs, lr=learning_rate, lr_decay_epoch=lr_decay_epoch ,threshold = threshold)\n","\n","trainer.train_and_evaluate()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5372944,"sourceId":8931416,"sourceType":"datasetVersion"}],"dockerImageVersionId":30775,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
